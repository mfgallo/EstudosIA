{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Trabalho de conclusão NLP</h1>\n",
    "\n",
    "<h2>Turma 10IA</h2>\n",
    "<h3>Membros:</h3>\n",
    "<b>Mauricio Gallo - RM: 336168</b></br>\n",
    "<b>Gustavo Villa - RM: 335645</b></br>\n",
    "<b>Rafael Favretti - RM: 335332</b></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura e análise dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_en</th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>Mais uma vez, o Sr. Costner arrumou um filme p...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>Este é um exemplo do motivo pelo qual a maiori...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>Primeiro de tudo eu odeio esses raps imbecis, ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>Nem mesmo os Beatles puderam escrever músicas ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Brass pictures movies is not a fitting word fo...</td>\n",
       "      <td>Filmes de fotos de latão não é uma palavra apr...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            text_en  \\\n",
       "0   1  Once again Mr. Costner has dragged out a movie...   \n",
       "1   2  This is an example of why the majority of acti...   \n",
       "2   3  First of all I hate those moronic rappers, who...   \n",
       "3   4  Not even the Beatles could write songs everyon...   \n",
       "4   5  Brass pictures movies is not a fitting word fo...   \n",
       "\n",
       "                                             text_pt sentiment  \n",
       "0  Mais uma vez, o Sr. Costner arrumou um filme p...       neg  \n",
       "1  Este é um exemplo do motivo pelo qual a maiori...       neg  \n",
       "2  Primeiro de tudo eu odeio esses raps imbecis, ...       neg  \n",
       "3  Nem mesmo os Beatles puderam escrever músicas ...       neg  \n",
       "4  Filmes de fotos de latão não é uma palavra apr...       neg  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import das bibliotecas\n",
    "import pandas as pd\n",
    "\n",
    "#Leitura do dataset\n",
    "df_review = pd.read_csv(\"https://dados-ml-pln.s3-sa-east-1.amazonaws.com/imdb-reviews-pt-br.csv\", sep = \",\", encoding='utf-8')\n",
    "\n",
    "#Visualização dos dados\n",
    "df_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id           49459\n",
      "text_en      49459\n",
      "text_pt      49459\n",
      "sentiment    49459\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Contagem dos dados no dataset\n",
    "print(df_review.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "text_en      0\n",
       "text_pt      0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checagem se há dados nulos no dataset\n",
    "df_review.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "text_en      0\n",
       "text_pt      0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checagem se há dados NA no dataset\n",
    "df_review.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49459, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificando as dimensões do dataset\n",
    "df_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    24765\n",
       "pos    24694\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Contagem inicial das classificações de sentimentos\n",
    "df_review.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contagem de palavras mais comuns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('de', 422154),\n",
       " ('que', 327261),\n",
       " ('e', 311782),\n",
       " ('o', 289805),\n",
       " ('a', 241726),\n",
       " ('um', 225146),\n",
       " ('é', 208453),\n",
       " ('não', 142163),\n",
       " ('em', 140213),\n",
       " ('uma', 136215)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counterPT = Counter()\n",
    "counterEN = Counter()\n",
    "\n",
    "#Contagem de palavras mais comuns em português\n",
    "df_review.text_pt.str.lower().str.split().apply(counterPT.update) \n",
    "counterPT.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 612098),\n",
       " ('a', 303745),\n",
       " ('and', 301853),\n",
       " ('of', 272250),\n",
       " ('to', 251218),\n",
       " ('is', 195962),\n",
       " ('in', 171570),\n",
       " ('i', 140594),\n",
       " ('this', 135274),\n",
       " ('it', 125757)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Contagem de palavras mais comuns em inglês\n",
    "df_review.text_en.str.lower().str.split().apply(counterEN.update) \n",
    "counterEN.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import das bibliotecas para geração dos tokens\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "#Criação de uma nova coluna no dataframe para armazenamento dos tokens e aplicação do Word Tokenize para sua geração\n",
    "df_review['tokens_pt'] = df_review.text_pt.apply(word_tokenize)\n",
    "df_review['tokens_en'] = df_review.text_en.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [Mais, uma, vez, ,, o, Sr., Costner, arrumou, ...\n",
       "1        [Este, é, um, exemplo, do, motivo, pelo, qual,...\n",
       "2        [Primeiro, de, tudo, eu, odeio, esses, raps, i...\n",
       "3        [Nem, mesmo, os, Beatles, puderam, escrever, m...\n",
       "4        [Filmes, de, fotos, de, latão, não, é, uma, pa...\n",
       "                               ...                        \n",
       "49454    [Como, a, média, de, votos, era, muito, baixa,...\n",
       "49455    [O, enredo, teve, algumas, reviravoltas, infel...\n",
       "49456    [Estou, espantado, com, a, forma, como, este, ...\n",
       "49457    [A, Christmas, Together, realmente, veio, ante...\n",
       "49458    [O, drama, romântico, da, classe, trabalhadora...\n",
       "Name: tokens_pt, Length: 49459, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokens PT\n",
    "df_review['tokens_pt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [Once, again, Mr., Costner, has, dragged, out,...\n",
       "1        [This, is, an, example, of, why, the, majority...\n",
       "2        [First, of, all, I, hate, those, moronic, rapp...\n",
       "3        [Not, even, the, Beatles, could, write, songs,...\n",
       "4        [Brass, pictures, movies, is, not, a, fitting,...\n",
       "                               ...                        \n",
       "49454    [Seeing, as, the, vote, average, was, pretty, ...\n",
       "49455    [The, plot, had, some, wretched, ,, unbelievab...\n",
       "49456    [I, am, amazed, at, how, this, movieand, most,...\n",
       "49457    [A, Christmas, Together, actually, came, befor...\n",
       "49458    [Working-class, romantic, drama, from, directo...\n",
       "Name: tokens_en, Length: 49459, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokens EN\n",
    "df_review['tokens_en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download das Stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "stopwords_pt = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "#Exemplo de stopwords\n",
    "stopwords_pt[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_en = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#Exemplo de stopwords\n",
    "stopwords_en[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import das bibliotecas\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.rslp import RSLPStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('rslp', quiet=True)\n",
    "\n",
    "#Instância Steamer para PT\n",
    "rslp = RSLPStemmer()\n",
    "\n",
    "#Instância Steamer para EN\n",
    "porter = PorterStemmer()\n",
    "\n",
    "#Função para aplicar steam aos tokens\n",
    "def stem_pandas_rslp(line):\n",
    "  return ' '.join([rslp.stem(token) for token in line])\n",
    "\n",
    "def stem_pandas_porter(line):\n",
    "  return ' '.join([porter.stem(token) for token in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemização PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    mais uma vez , o sr. costn arrum um film por m...\n",
       "1    est é um exempl do motiv pel qual a maior do f...\n",
       "2    prim de tud eu odei ess rap imbecil , que não ...\n",
       "3    nem mesm os beatl pud escrev músic que tod gos...\n",
       "4    film de fot de lat não é uma palavr apropri pa...\n",
       "Name: stemmer_pt, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review['stemmer_pt'] = df_review.tokens_pt.apply(stem_pandas_rslp)\n",
    "\n",
    "df_review.stemmer_pt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemização EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    onc again mr. costner ha drag out a movi for f...\n",
       "1    thi is an exampl of whi the major of action fi...\n",
       "2    first of all I hate those moron rapper , who c...\n",
       "3    not even the beatl could write song everyon li...\n",
       "4    brass pictur movi is not a fit word for them r...\n",
       "Name: stemmer_en, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review['stemmer_en'] = df_review.tokens_en.apply(stem_pandas_porter)\n",
    "\n",
    "df_review.stemmer_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contagem de unigramas com CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilização com Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texto em PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade unigramas texto PT 77831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect_pt_stop = CountVectorizer(ngram_range=(1,1), stop_words=stopwords_pt)\n",
    "#vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect_pt_stop.fit(df_review.stemmer_pt)\n",
    "count_vect_pt_stop = vect_pt_stop.transform(df_review.stemmer_pt)\n",
    "\n",
    "print('Quantidade unigramas texto PT', count_vect_pt_stop.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texto em EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade unigramas texto EN 87576\n"
     ]
    }
   ],
   "source": [
    "vect_en_stop = CountVectorizer(ngram_range=(1,1), stop_words=stopwords_en)\n",
    "#vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect_en_stop.fit(df_review.stemmer_en)\n",
    "count_vect_en_stop = vect_en_stop.transform(df_review.stemmer_en)\n",
    "\n",
    "print('Quantidade unigramas texto EN', count_vect_en_stop.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilização sem Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texto em PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade unigramas texto PT 77937\n"
     ]
    }
   ],
   "source": [
    "vect_pt = CountVectorizer(ngram_range=(1,1))\n",
    "#vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect_pt.fit(df_review.stemmer_pt)\n",
    "count_vect_pt = vect_pt.transform(df_review.stemmer_pt)\n",
    "\n",
    "print('Quantidade unigramas texto PT', count_vect_pt.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texto em EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade unigramas texto EN 87718\n"
     ]
    }
   ],
   "source": [
    "vect_en = CountVectorizer(ngram_range=(1,1))\n",
    "#vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect_en.fit(df_review.stemmer_en)\n",
    "count_vect_en = vect_en.transform(df_review.stemmer_en)\n",
    "                                  \n",
    "print('Quantidade unigramas texto EN', count_vect_en.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultado da análise de stopwords com unigramas\n",
    "\n",
    "Após essa análise verificamos que a utilização ou não de stop words para unigramas não produz uma redução significativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contagem de bigramas com CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilização com Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texto em PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade bigramas texto PT 2327600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect_pt_stop_bi = CountVectorizer(ngram_range=(2,2), stop_words=stopwords_pt)\n",
    "vect_pt_stop_bi.fit(df_review.stemmer_pt)\n",
    "count_vect_pt_stop_bi = vect_pt_stop_bi.transform(df_review.stemmer_pt)\n",
    "\n",
    "print('Quantidade bigramas texto PT', count_vect_pt_stop_bi.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texto em EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade bigramas texto EN 2548514\n"
     ]
    }
   ],
   "source": [
    "vect_en_stop_bi = CountVectorizer(ngram_range=(2,2), stop_words=stopwords_en)\n",
    "vect_en_stop_bi.fit(df_review.stemmer_en)\n",
    "count_vect_en_stop_bi = vect_en_stop_bi.transform(df_review.stemmer_en)\n",
    "\n",
    "print('Quantidade bigramas texto EN', count_vect_en_stop_bi.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilização sem Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texto em PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade bigramas texto PT 1855113\n"
     ]
    }
   ],
   "source": [
    "vect_pt_bi = CountVectorizer(ngram_range=(2,2))\n",
    "vect_pt_bi.fit(df_review.stemmer_pt)\n",
    "count_vect_pt_bi = vect_pt_bi.transform(df_review.stemmer_pt)\n",
    "\n",
    "print('Quantidade bigramas texto PT', count_vect_pt_bi.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texto em EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade bigramas texto EN 2005774\n"
     ]
    }
   ],
   "source": [
    "vect_en_bi = CountVectorizer(ngram_range=(2,2))\n",
    "vect_en_bi.fit(df_review.stemmer_en)\n",
    "count_vect_en_bi = vect_en_bi.transform(df_review.stemmer_en)\n",
    "                                  \n",
    "print('Quantidade bigramas texto EN', count_vect_en_bi.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultado da análise de stopwords com bigramas\n",
    "\n",
    "Após essa análise verificamos que a utilização de stop words para bigramas produz uma redução significativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import da biblioteca\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<39567x77831 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4055771 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cópia do dataset\n",
    "df_uni_pt = df_review\n",
    "\n",
    "#Separação entre teste e treino\n",
    "X_train_uni_pt, X_test_uni_pt, y_train_uni_pt, y_test_uni_pt = train_test_split(\n",
    "count_vect_pt_stop,\n",
    "df_uni_pt[\"sentiment\"],\n",
    "test_size = 0.2,\n",
    "random_state = 42)\n",
    "\n",
    "X_train_uni_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=42, splitter='best')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Treino da árvore de decisão\n",
    "tree_uni_pt = DecisionTreeClassifier(random_state = 42)\n",
    "tree_uni_pt.fit(X_train_uni_pt, y_train_uni_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7164375252729478"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Avaliação da acurácia do modelo\n",
    "tree_uni_pt.score(X_test_uni_pt, y_test_uni_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<39567x87576 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3851756 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cópia do dataset\n",
    "df_uni_en = df_review\n",
    "\n",
    "#Separação entre teste e treino\n",
    "X_train_uni_en, X_test_uni_en, y_train_uni_en, y_test_uni_en = train_test_split(\n",
    "count_vect_en_stop,\n",
    "df_uni_en[\"sentiment\"],\n",
    "test_size = 0.2,\n",
    "random_state = 42)\n",
    "\n",
    "X_train_uni_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=42, splitter='best')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Treino da árvore de decisão\n",
    "tree_uni_en = DecisionTreeClassifier(random_state = 42)\n",
    "tree_uni_en.fit(X_train_uni_en, y_train_uni_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7308936514355034"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Avaliação da acurácia do modelo\n",
    "tree_uni_en.score(X_test_uni_en, y_test_uni_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigramas\n",
    "\n",
    "### PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<39567x2975485 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4885457 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cópia do dataset\n",
    "df_bi_pt = df_review\n",
    "\n",
    "#Separação entre teste e treino\n",
    "X_train_bi_pt, X_test_bi_pt, y_train_bi_pt, y_test_bi_pt = train_test_split(\n",
    "count_vect_pt_stop_bi,\n",
    "df_bi_pt[\"sentiment\"],\n",
    "test_size = 0.2,\n",
    "random_state = 42)\n",
    "\n",
    "X_train_bi_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=42, splitter='best')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Treino da árvore de decisão\n",
    "tree_bi_pt = DecisionTreeClassifier(random_state = 42)\n",
    "tree_bi_pt.fit(X_train_bi_pt, y_train_bi_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7354427820460978"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Avaliação da acurácia do modelo\n",
    "tree_bi_pt.score(X_test_bi_pt, y_test_bi_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<39567x2548514 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4812369 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cópia do dataset\n",
    "df_bi_en = df_review\n",
    "\n",
    "#Separação entre teste e treino\n",
    "X_train_bi_en, X_test_bi_en, y_train_bi_en, y_test_bi_en = train_test_split(\n",
    "count_vect_en_stop_bi,\n",
    "df_bi_en[\"sentiment\"],\n",
    "test_size = 0.2,\n",
    "random_state = 42)\n",
    "\n",
    "X_train_bi_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=42, splitter='best')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Treino da árvore de decisão\n",
    "tree_bi_en = DecisionTreeClassifier(random_state = 42)\n",
    "tree_bi_en.fit(X_train_bi_en, y_train_bi_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7382733522038011"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Avaliação da acurácia do modelo\n",
    "tree_bi_en.score(X_test_bi_en, y_test_bi_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise com stopwords Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import da biblioteca Spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "499\n"
     ]
    }
   ],
   "source": [
    "#Quantidades de stopwords\n",
    "print(len(stopwords_pt))\n",
    "\n",
    "#Pegando stopwords do spacy\n",
    "stopwords_space_pt = spacy.load('pt_core_news_sm').Defaults.stop_words\n",
    "\n",
    "#Lista stopwords concatenada\n",
    "stopwords_full_pt = set(stopwords_space_pt).union(set(stopwords_pt))\n",
    "\n",
    "#Count de stopwords atual\n",
    "print(len(stopwords_full_pt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após isso resolvemos executar novamente o modelo com bigramas pois foi o mais efetivo até o momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade bigramas texto PT 2280339\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect_pt_stop_bi = CountVectorizer(ngram_range=(2,2), stop_words=stopwords_full_pt)\n",
    "vect_pt_stop_bi.fit(df_review.stemmer_pt)\n",
    "count_vect_pt_stop_bi = vect_pt_stop_bi.transform(df_review.stemmer_pt)\n",
    "\n",
    "print('Quantidade bigramas texto PT', count_vect_pt_stop_bi.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<39567x2280339 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4836559 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cópia do dataset\n",
    "df_bi_pt = df_review\n",
    "\n",
    "#Separação entre teste e treino\n",
    "X_train_bi_pt, X_test_bi_pt, y_train_bi_pt, y_test_bi_pt = train_test_split(\n",
    "count_vect_pt_stop_bi,\n",
    "df_bi_pt[\"sentiment\"],\n",
    "test_size = 0.2,\n",
    "random_state = 42)\n",
    "\n",
    "X_train_bi_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=42, splitter='best')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Treino da árvore de decisão\n",
    "tree_bi_pt = DecisionTreeClassifier(random_state = 42)\n",
    "tree_bi_pt.fit(X_train_bi_pt, y_train_bi_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7197735543873838"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Avaliação da acurácia do modelo\n",
    "tree_bi_pt.score(X_test_bi_pt, y_test_bi_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "382\n"
     ]
    }
   ],
   "source": [
    "#Quantidades de stopwords\n",
    "print(len(stopwords_en))\n",
    "\n",
    "#Pegando stopwords do spacy\n",
    "stopwords_space_en = spacy.load('en_core_web_md').Defaults.stop_words\n",
    "\n",
    "#Lista stopwords concatenada\n",
    "stopwords_full_en = set(stopwords_space_en).union(set(stopwords_en))\n",
    "\n",
    "#Count de stopwords atual\n",
    "print(len(stopwords_full_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade bigramas texto EN 2463386\n"
     ]
    }
   ],
   "source": [
    "vect_en_stop_bi = CountVectorizer(ngram_range=(2,2), stop_words=stopwords_full_en)\n",
    "vect_en_stop_bi.fit(df_review.stemmer_en)\n",
    "count_vect_en_stop_bi = vect_en_stop_bi.transform(df_review.stemmer_en)\n",
    "\n",
    "print('Quantidade bigramas texto EN', count_vect_en_stop_bi.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<39567x2463386 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4306741 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cópia do dataset\n",
    "df_bi_en = df_review\n",
    "\n",
    "#Separação entre teste e treino\n",
    "X_train_bi_en, X_test_bi_en, y_train_bi_en, y_test_bi_en = train_test_split(\n",
    "count_vect_en_stop_bi,\n",
    "df_bi_en[\"sentiment\"],\n",
    "test_size = 0.2,\n",
    "random_state = 42)\n",
    "\n",
    "X_train_bi_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=42, splitter='best')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Treino da árvore de decisão\n",
    "tree_bi_en = DecisionTreeClassifier(random_state = 42)\n",
    "tree_bi_en.fit(X_train_bi_en, y_train_bi_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7287707238172261"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Avaliação da acurácia do modelo\n",
    "tree_bi_en.score(X_test_bi_en, y_test_bi_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_pt = spacy.load('pt_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_md')\n",
    "\n",
    "#Função para realizar a lematização em pt\n",
    "def lemmatizer_text_pt(text):        \n",
    "  sent = []\n",
    "  doc = spacy_pt(text)\n",
    "  for word in doc:\n",
    "      sent.append(word.lemma_)\n",
    "  return \" \".join(sent)\n",
    "\n",
    "\n",
    "#Função para realizar a lematização em en\n",
    "def lemmatizer_text_en(text):        \n",
    "  sent = []\n",
    "  doc = spacy_en(text)\n",
    "  for word in doc:\n",
    "      sent.append(word.lemma_)\n",
    "  return \" \".join(sent)\n",
    "\n",
    "df_review['text_lemma_pt'] = df_review.text_pt.apply(lemmatizer_text_pt)\n",
    "df_review['text_lemma_en'] = df_review.text_en.apply(lemmatizer_text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Mais umar vez , o Sr. Costner arrumar um filma...\n",
      "1    Este ser um exemplo do motivar pelar qual o ma...\n",
      "2    Primeiro de tudo eu odiar esse rap imbecil , q...\n",
      "3    Nem mesmo o Beatles poder escrever músico que ...\n",
      "4    Filmes de foto de latão não ser umar palavra a...\n",
      "Name: text_lemma_pt, dtype: object\n",
      "0    once again Mr. Costner have drag out a movie f...\n",
      "1    this be an example of why the majority of acti...\n",
      "2    first of all -PRON- hate those moronic rapper ...\n",
      "3    not even the Beatles could write song everyone...\n",
      "4    brass picture movie be not a fitting word for ...\n",
      "Name: text_lemma_en, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Checando o head da co\n",
    "print(df_review['text_lemma_pt'].head())\n",
    "print(df_review['text_lemma_en'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Copia do datasource\n",
    "df_spacy_lemma = df_review\n",
    "\n",
    "vect_spacy_lemma = CountVectorizer(ngram_range=(1,2), stop_words=stopwords_full_pt) # exemplo 2.1: vetorização e combinação de unigrama e bigrama sem stopwords\n",
    "vect_spacy_lemma.fit(df_spacy_lemma.text_lemma_pt)\n",
    "text_vect_spacy_lemma = vect_spacy_lemma.transform(df_spacy_lemma.text_lemma_pt)\n",
    "\n",
    "#Separação dos dados\n",
    "X_train_spacy_lemma,X_test_spacy_lemma,y_train_spacy_lemma,y_test_spacy_lemma = train_test_split(\n",
    "    text_vect_spacy_lemma, \n",
    "    df_spacy_lemma[\"sentiment\"], \n",
    "    test_size = 0.2,\n",
    "    random_state = 42\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=42, splitter='best')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Treino do modelo\n",
    "spacy_tree = DecisionTreeClassifier(random_state=42)\n",
    "spacy_tree.fit(X_train_spacy_lemma, y_train_spacy_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7259401536595228\n"
     ]
    }
   ],
   "source": [
    "#Teste acurácia\n",
    "y_prediction = spacy_tree.predict(X_test_spacy_lemma)\n",
    "\n",
    "accuracy = accuracy_score(y_prediction, y_test_spacy_lemma)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste sem stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copia do datasource\n",
    "df_spacy_lemma = df_review\n",
    "\n",
    "vect_spacy_lemma = CountVectorizer(ngram_range=(1,2))\n",
    "vect_spacy_lemma.fit(df_spacy_lemma.text_lemma_pt)\n",
    "text_vect_spacy_lemma = vect_spacy_lemma.transform(df_spacy_lemma.text_lemma_pt)\n",
    "\n",
    "#Separação dos dados\n",
    "X_train_spacy_lemma,X_test_spacy_lemma,y_train_spacy_lemma,y_test_spacy_lemma = train_test_split(\n",
    "    text_vect_spacy_lemma, \n",
    "    df_spacy_lemma[\"sentiment\"], \n",
    "    test_size = 0.2,\n",
    "    random_state = 42\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=42, splitter='best')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Treino do modelo\n",
    "spacy_tree = DecisionTreeClassifier(random_state=42)\n",
    "spacy_tree.fit(X_train_spacy_lemma, y_train_spacy_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.727456530529721\n"
     ]
    }
   ],
   "source": [
    "#Teste acurácia\n",
    "y_prediction = spacy_tree.predict(X_test_spacy_lemma)\n",
    "\n",
    "accuracy = accuracy_score(y_prediction, y_test_spacy_lemma)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão\n",
    "\n",
    "Concluímos que o melhor cenário a ser utilizado é execução de tokenização com bigramas, resultando em uma acurácia de 73% tanto para português quanto para inglês."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
